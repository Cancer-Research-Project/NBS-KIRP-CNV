###############################################
# ---------- Data Import Functions ---------- #
###############################################

import pandas as pd
import numpy as np
import networkx as nx
import time
import os
import random

# Load network from file as unweighted network
# params is generated by the load_params() function
# Can set delimiter, but default delimiter is tab
# Only will read edges as first two columns, all other columns will be ignored
# There are also options to shuffle the network to be loaded if desired (testing randomized network controls)


def load_network_file(network_file_path, delimiter='\t', degree_shuffle=False, label_shuffle=False, verbose=True):
    # Load network using networkx
    network = nx.read_edgelist(
        network_file_path, delimiter=delimiter, data=False)
    if verbose:
        print('Network File Loaded:', network_file_path)
    if degree_shuffle:
        network = degree_shuffNet(network, verbose=verbose)
    if label_shuffle:
        network = label_shuffNet(network, verbose=verbose)
    return network

# Load binary mutation data with 2 file types (filetype= 'matrix' or 'list')
# params is a dictionary of optional function parameters. See the Github wiki for additional details on these parameters.
# params is generated by the load_params() function
# filetype=='matrix' is a csv or tsv style matrix with row and column headers, rows are samples/patients, columns are genes
# filetype=='list' is a 2 columns text file separated by the delimiter where 1st column is sample/patient, 2nd column is one gene mutated in that patient
# Line example in 'list' file: 'Patient ID','Gene Mutated'


def load_binary_mutation_data(filename, filetype='list', delimiter='\t', verbose=True):
    # Load binary mutation data from file
    if filetype == 'list':
        f = open(filename)
        binary_mat_lines = f.read().splitlines()
        binary_mat_data = [(line.split('\t')[0], line.split('\t')[1])
                           for line in binary_mat_lines]
        binary_mat_index = pd.MultiIndex.from_tuples(
            binary_mat_data, names=['Tumor_Sample_Barcode', 'Gene_Name'])
        binary_mat_2col = pd.DataFrame(
            1, index=binary_mat_index, columns=[0])[0]
        binary_mat = binary_mat_2col.unstack().fillna(0)
    elif filetype == 'matrix':
        binary_mat = pd.read_csv(
            filename, delimiter=delimiter, index_col=0).astype(int)
    else:
        raise ValueError("'filetype' must be either 'matrix' or 'list'.")
    if verbose:
        print('Binary Mutation Matrix Loaded:', filename)
    return binary_mat

# Loads any non-default optional parameters that may be used in the pyNBS pipeline
# If no file is given, all default values will be set
# If only some parameters are given in parameter file, default values will be used for all other parameters.
# Full parameter documentation is given in the GitHub wiki documentation


def load_params(params_file=None):
    run_pyNBS_params = {
        # Overall pyNBS Parameters
        'verbose': True,
        'job_name': 'pyNBS',
        'outdir': './Results/',
        # Data Loading Parameters
        'mut_filetype': 'list',
        'mut_filedelim': '\t',
        'net_filedelim': '\t',
        'degree_preserved_shuffle': False,
        'node_label_shuffle': False,
        # Data Subsampling Parameters
        'pats_subsample_p': 0.8,
        'gene_subsample_p': 0.8,
        'min_muts': 10,
        # Network Propagation Parameters
        'prop_data': True,
        'prop_alpha': 0.7,
        'prop_symmetric_norm': False,
        'save_kernel': False,
        'save_prop': False,
        'qnorm_data': True,
        # KNN Network Construction Parameters
        'reg_net_gamma': 0.01,
        'k_nearest_neighbors': 11,
        'save_knn_glap': True,
        # Network Regularized NMF Parameters
        'netNMF_k': 4,
        'netNMF_lambda': 200,
        'netNMF_maxiter': 250,
        'netNMF_eps': 1e-15,
        'netNMF_err_tol': 1e-4,
        'netNMF_err_delta_tol': 1e-8,
        'save_H': False,
        # Consensus Clustering Parameters
        'niter': 100,
        'hclust_linkage_method': 'average',
        'hclust_linkage_metric': 'euclidean',
        'save_cc_results': True,
        'save_cc_map': True,
        # Cluster Survival Analysis Parameters
        'plot_survival': False,
        'surv_file_delim': '\t',
        'surv_lr_test': True,
        'surv_tmax': -1,
        'save_KM_plot': False
    }
    # Load parameters from file and change any given values
    # Performs data type assignment for parameters, throws errors if types cannot be cast
    if params_file is not None:
        params_file = pd.read_csv(
            params_file, header=-1, comment='#', index_col=0)
        params_file.columns = ['value']
        for param in params_file.index:
            if param in run_pyNBS_params:
                if type(run_pyNBS_params[param]) == bool:
                    run_pyNBS_params[param] = (
                        params_file.loc[param, 'value'] == 'True')
                else:
                    run_pyNBS_params[param] = params_file.loc[param].astype(
                        type(run_pyNBS_params[param]))['value']
            else:
                run_pyNBS_params[param] = params_file['value'].loc[param]
    else:
        pass
    # Constructs output directory if directory does not exist
    if not os.path.exists(run_pyNBS_params['outdir']):
        os.makedirs(run_pyNBS_params['outdir'])
    return run_pyNBS_params

# Shuffle network by preserving node-degree


def degree_shuffNet(network, verbose=False):
    shuff_time = time.time()
    edge_len = len(list(network.edges))
    shuff_net = network.copy()
    try:
        nx.double_edge_swap(shuff_net, nswap=edge_len, max_tries=edge_len*10)
    except:
        if verbose:
            print('Note: Maximum number of swap attempts ('+repr(edge_len*10) +
                  ') exceeded before desired swaps achieved ('+repr(edge_len)+').')
    if verbose:
        # Evaluate Network Similarity
        shared_edges = len(set(list(network.edges)).intersection(
            set(list(shuff_net.edges))))
        print('Network shuffled:', time.time()-shuff_time,
              'seconds. Edge similarity:', shared_edges/float(edge_len))
    return shuff_net

# Shuffle network by permuting network node labels


def label_shuffNet(network, verbose=False):
    shuff_time = time.time()
    edge_len = len(list(network.edges))
    # Permute node labels
    network_nodes = list(network.nodes)
    shuff_nodes = list(network_nodes)
    for i in range(10):
        random.shuffle(shuff_nodes)
    network_relabel_map = {
        network_nodes[i]: shuff_nodes[i] for i in range(len(network_nodes))}
    shuff_net = nx.relabel_nodes(network, network_relabel_map, copy=True)
    if verbose:
        # Evaluate Network Similarity
        shared_edges = len(set(list(network.edges)).intersection(
            set(list(shuff_net.edges))))
        print('Network shuffled:', time.time()-shuff_time,
              'seconds. Edge similarity:', shared_edges/float(edge_len))
    return shuff_net

# Filter extended network txt file where all edges are weighted by a specific quantile
# Return the filtered network edge list and save it to a file if desired (for import by load_network_file)
# The input weighted network file may be any table format of edge list, but the columns for Node A, Node B, and weight must be specified


def filter_weighted_network(network_file_path, save_path, nodeA_col=0, nodeB_col=1, score_col=2, q=0.9, delimiter='\t', verbose=False):
    data = pd.read_csv(network_file_path, sep=delimiter,
                       header=-1, low_memory=False)
    # Filter edges by score quantile
    q_score = data[score_col].quantile(q)
    if verbose:
        print(str(round(q*100, 2))+'%', 'score:', q_score)
    data_filt = data[data[score_col] >
                     q_score][data.columns[[nodeA_col, nodeB_col, score_col]]]
    data_filt.columns = ['nodeA', 'nodeB', 'edgeScore']
    if verbose:
        print(data_filt.shape[0], '/', data.shape[0], 'edges retained')
    data_filt.to_csv(save_path, sep='\t', header=False, index=False)
    return

# Convert and save MAF from Broad Firehose
# Can produce 2 types of filetypes: 'matrix' or 'list', matrix is a full samples-by-genes binary csv, 'list' is a sparse representation of 'matrix'
# This is a conversion tool, so the result must be saved (most tools will require a path to a processed MAF file and load it separately)
# Gene naming can be 'Symbol' or 'Entrez'


def process_TCGA_MAF(maf_file, save_path, filetype='matrix', gene_naming='Symbol', verbose=False):
    loadtime = time.time()
    # Load MAF File
    TCGA_MAF = pd.read_csv(maf_file, sep='\t', low_memory=False)
    # Get all patient somatic mutation (sm) pairs from MAF file
    if gene_naming == 'Entrez':
        TCGA_sm = TCGA_MAF.groupby(
            ['Tumor_Sample_Barcode', 'Entrez_Gene_Id']).size()
    else:
        TCGA_sm = TCGA_MAF.groupby(
            ['Tumor_Sample_Barcode', 'Hugo_Symbol']).size()
    # Turn somatic mutation data into binary matrix
    TCGA_sm_mat = TCGA_sm.unstack().fillna(0)
    TCGA_sm_mat = (TCGA_sm_mat > 0).astype(int)
    # Trim TCGA barcodes
    TCGA_sm_mat.index = [pat[:12] for pat in TCGA_sm_mat.index]
    # Filter samples with duplicate IDs
    non_dup_IDs = list(TCGA_sm_mat.index.value_counts(
    ).index[TCGA_sm_mat.index.value_counts() == 1])
    dup_IDs = list(TCGA_sm_mat.index.value_counts(
    ).index[TCGA_sm_mat.index.value_counts() > 1])
    # Save file as binary matrix or sparse list
    if filetype == 'list':
        # Now try to construct two-column/sparse representation of binary sm data
        # Get list of all patient somatic mutations
        index_list = list(TCGA_sm.index)
        # Filter list of patient somatic mutations of duplicate patient barcodes
        index_list_filt = [i for i in index_list if not any(
            [True if barcode in i[0] else False for barcode in dup_IDs])]
        # Save patient somatic mutations list to file
        f = open(save_path, 'w')
        for sm in index_list_filt:
            f.write(sm[0][:12]+'\t'+sm[1]+'\n')
        f.close()
        if verbose:
            print('Binary somatic mutations list saved')
    else:
        # Save non-duplicate patients' binary TCGA somatic mutation matrix to csv
        TCGA_sm_mat_filt = TCGA_sm_mat.loc[non_dup_IDs]
        # Remove all genes that have no more mutations after patient filtering
        nonempty_cols = [col for col in TCGA_sm_mat_filt.columns if not all(
            TCGA_sm_mat_filt[col] == 0)]
        TCGA_sm_mat_filt2 = TCGA_sm_mat_filt[nonempty_cols]
        # Remove columns with bad names like '0'
        named_cols = [col for col in TCGA_sm_mat_filt.columns if col != '0']
        TCGA_sm_mat_filt3 = TCGA_sm_mat_filt2[nonempty_cols]
        TCGA_sm_mat_filt3.to_csv(save_path)
        if verbose:
            print('Binary somatic mutation matrix saved')
    if verbose:
        print('MAF file processed:', maf_file, round(
            time.time()-loadtime, 2), 'seconds.')
    return
